{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n",
      "+------+--------------------+-----+\n",
      "|    id|            features|label|\n",
      "+------+--------------------+-----+\n",
      "|205661|(20,[0,1,2,3,4,5,...|    0|\n",
      "|206148|(20,[0,1,2,3,4,5,...|    0|\n",
      "|206369|(20,[13],[0.33957...|    0|\n",
      "|206436|(20,[0,1,2,3,4,5,...|    0|\n",
      "|207119|(20,[0,1,2,3,4,5,...|    0|\n",
      "|207153|(20,[0,1,2,3,4,5,...|    0|\n",
      "|207214|(20,[0,1,2,3,4,5,...|    0|\n",
      "|207437|(20,[0,1,2,3,4,5,...|    0|\n",
      "|207482|(20,[0,1,2,3,4,5,...|    0|\n",
      "|207735|(20,[0,1,2,3,4,5,...|    0|\n",
      "|208088|(20,[0,1,2,3,4,5,...|    0|\n",
      "|208519|(20,[0,1,2,3,4,5,...|    0|\n",
      "|208803|(20,[0,1,2,3,4,5,...|    0|\n",
      "|208933|(20,[0,1,2,3,4,5,...|    0|\n",
      "|209059|(20,[0,1,2,3,4,5,...|    0|\n",
      "|209467|(20,[0,1,2,3,4,5,...|    0|\n",
      "|209844|(20,[0,1,2,3,4,5,...|    0|\n",
      "|210549|(20,[0,2,3,4,5,6,...|    0|\n",
      "|210940|(20,[0,1,2,3,4,5,...|    0|\n",
      "|210997|(20,[0,1,2,3,4,5,...|    0|\n",
      "+------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SQLContext\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Clustering\")\\\n",
    "    .getOrCreate()\n",
    "# dataframe1 = spark.read.json('hdfs://localhost:1234/user/tl2861/hw3/train.json')\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "# dataframe1 = sqlContext.read.json('hdfs://localhost:1234/user/tl2861/hw3/train.json')\n",
    "dataframe1 = sqlContext.read.json('../../../data/AA/wiki_*').withColumn('label',lit(0))\n",
    "dataframe2 = sqlContext.read.json('../../../data/AAquote/wiki_*').withColumn('label', lit(1))\n",
    "# dataframe3 = sqlContext.read.json('../../data/AAvoyage/wiki_*').withColumn('label',lit(3))\n",
    "dataframe = dataframe1.unionAll(dataframe2)\n",
    "# dataframe1.printSchema()\n",
    "# dataframe2.printSchema()\n",
    "dataframe.printSchema()\n",
    "# dataframe1.show()\n",
    "#regexTokenizer = RegexTokenizer()\n",
    "sampledData = dataframe.sampleBy(\"label\", fractions={0: 0.2, 1: 0.2}, seed=0)\n",
    "# sampledData.show()\n",
    "# sampledData0 = pd.DataFrame(sampledData)\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"[^A-Za-z]+\", toLowercase=True)\n",
    "tokenizedData = regexTokenizer.transform(sampledData)\n",
    "\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filteredData = stopWordsRemover.transform(tokenizedData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(filteredData)\n",
    "\n",
    "idf= IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "data1 = idfModel.transform(featurizedData)\n",
    "\n",
    "datatext = data1.select('id','features','label')\n",
    "datatext.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopWordsRemover, hashingTF,idf])\n",
    "pipeline_model=pipeline.fit(sampledData)\n",
    "processed_data = pipeline_model.transform(sampledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0246abbcc440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatatext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatatext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDenseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "datatext = datatext.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatext.toPandas().to_csv(\"../1/dataCluster.csv\", sep = \",\", index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 1335214.9809889607\n",
      "Cluster Centers: \n",
      "[24.70595957 21.86310698 28.09276079 24.66477462 31.0944174  18.56860122\n",
      " 22.48443747 19.4617212  23.95028956 18.58052216 23.5488747  21.43006121\n",
      " 17.48836579 26.64247329 20.18451575 24.49362609 19.96815644 22.04731022\n",
      " 19.89367364 21.86714012]\n",
      "[1.45820232 1.27541215 1.43199953 1.33120029 1.53704863 1.16367569\n",
      " 1.33369765 1.24050032 1.49248339 1.28095416 1.32525024 1.31310688\n",
      " 1.16756241 1.47920655 1.32047555 1.39996726 1.30438661 1.2579419\n",
      " 1.25805494 1.19740135]\n",
      "+------+--------------------+----------+\n",
      "|    id|            features|prediction|\n",
      "+------+--------------------+----------+\n",
      "|205661|(20,[0,1,2,3,4,5,...|         1|\n",
      "|206148|(20,[0,1,2,3,4,5,...|         1|\n",
      "|206369|(20,[13],[0.33957...|         1|\n",
      "|206436|(20,[0,1,2,3,4,5,...|         1|\n",
      "|207119|(20,[0,1,2,3,4,5,...|         1|\n",
      "|207153|(20,[0,1,2,3,4,5,...|         1|\n",
      "|207214|(20,[0,1,2,3,4,5,...|         1|\n",
      "|207437|(20,[0,1,2,3,4,5,...|         1|\n",
      "|207482|(20,[0,1,2,3,4,5,...|         1|\n",
      "|207735|(20,[0,1,2,3,4,5,...|         1|\n",
      "|208088|(20,[0,1,2,3,4,5,...|         1|\n",
      "|208519|(20,[0,1,2,3,4,5,...|         1|\n",
      "|208803|(20,[0,1,2,3,4,5,...|         1|\n",
      "|208933|(20,[0,1,2,3,4,5,...|         1|\n",
      "|209059|(20,[0,1,2,3,4,5,...|         1|\n",
      "|209467|(20,[0,1,2,3,4,5,...|         1|\n",
      "|209844|(20,[0,1,2,3,4,5,...|         1|\n",
      "|210549|(20,[0,2,3,4,5,6,...|         1|\n",
      "|210940|(20,[0,1,2,3,4,5,...|         1|\n",
      "|210997|(20,[0,1,2,3,4,5,...|         1|\n",
      "+------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans().setK(2).setSeed(4381)\n",
    "kmModel = kmeans.fit(data1)\n",
    "wssse = kmModel.computeCost(data1)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "     print(center)\n",
    "        \n",
    "data_kmeans = kmModel.transform(data1).select('id','features','prediction')\n",
    "data_kmeans.show()\n",
    "data_kmeans.toPandas().to_csv(\"../1/dataCluster-kmeans.csv\",sep = \",\", index = False, encoding = \"utf-8\")       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
