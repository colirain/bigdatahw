{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clustering\n",
    "#### 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "|    id|                text|               title|                 url|label|\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "|204805|BP report into Gu...|BP report into Gu...|https://en.wikine...|    1|\n",
      "|204818|200 candles: Chil...|200 candles: Chil...|https://en.wikine...|    1|\n",
      "|204838|New flotilla plan...|New flotilla plan...|https://en.wikine...|    1|\n",
      "|205039|Large gas main ex...|Large gas main ex...|https://en.wikine...|    1|\n",
      "|205165|Air Zimbabwe pilo...|Air Zimbabwe pilo...|https://en.wikine...|    1|\n",
      "|205171|Police may have k...|Police may have k...|https://en.wikine...|    1|\n",
      "|205326|Test article for ...|Test article for ...|https://en.wikine...|    1|\n",
      "|205328|Another test arti...|Another test arti...|https://en.wikine...|    1|\n",
      "|205340|Japanese motorcyl...|Japanese motorcyl...|https://en.wikine...|    1|\n",
      "|205345|Up to ten reporte...|Up to ten reporte...|https://en.wikine...|    1|\n",
      "|205355|Copenhagen hotel ...|Copenhagen hotel ...|https://en.wikine...|    1|\n",
      "|205528|Nokia appoints Mi...|Nokia appoints Mi...|https://en.wikine...|    1|\n",
      "|205578|American professi...|American professi...|https://en.wikine...|    1|\n",
      "|205661|Australian rules ...|Australian rules ...|https://en.wikine...|    1|\n",
      "|206018|Headless man foun...|Headless man foun...|https://en.wikine...|    1|\n",
      "|206042|Tea Party-endorse...|Tea Party-endorse...|https://en.wikine...|    1|\n",
      "|206148|Pope Benedict XVI...|Pope Benedict XVI...|https://en.wikine...|    1|\n",
      "|206230|The Club steering...|The Club steering...|https://en.wikine...|    1|\n",
      "|206241|Six die in US-Ira...|Six die in US-Ira...|https://en.wikine...|    1|\n",
      "|206259|Tulsa media erron...|Tulsa media erron...|https://en.wikine...|    1|\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "|    id|                text|               title|                 url|label|\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "|205661|Australian rules ...|Australian rules ...|https://en.wikine...|    1|\n",
      "|206148|Pope Benedict XVI...|Pope Benedict XVI...|https://en.wikine...|    1|\n",
      "|206369|           Victoria\n",
      "|            Victoria|https://en.wikine...|    1|\n",
      "|206436|Gunman shoots doc...|Gunman shoots doc...|https://en.wikine...|    1|\n",
      "|207119|Pope Benedict XVI...|Pope Benedict XVI...|https://en.wikine...|    1|\n",
      "|207153|Deepwater Horizon...|Deepwater Horizon...|https://en.wikine...|    1|\n",
      "|207214|Thirty die in car...|Thirty die in car...|https://en.wikine...|    1|\n",
      "|207437|Emma's Imaginatio...|Emma's Imaginatio...|https://en.wikine...|    1|\n",
      "|207482|Common cold virus...|Common cold virus...|https://en.wikine...|    1|\n",
      "|207735|New Zealand man g...|New Zealand man g...|https://en.wikine...|    1|\n",
      "|208088|'Poetry lost': ru...|'Poetry lost': ru...|https://en.wikine...|    1|\n",
      "|208519|Man released foll...|Man released foll...|https://en.wikine...|    1|\n",
      "|208803|Fernando Alonso w...|Fernando Alonso w...|https://en.wikine...|    1|\n",
      "|208933|Owner of Segway J...|Owner of Segway J...|https://en.wikine...|    1|\n",
      "|209059|Southwest Airline...|Southwest Airline...|https://en.wikine...|    1|\n",
      "|209467|Wikinews Shorts: ...|Wikinews Shorts: ...|https://en.wikine...|    1|\n",
      "|209844|US undergraduate ...|US undergraduate ...|https://en.wikine...|    1|\n",
      "|210549|Plane crash on Ca...|Plane crash on Ca...|https://en.wikine...|    1|\n",
      "|210940|Bomb attack in Lo...|Bomb attack in Lo...|https://en.wikine...|    1|\n",
      "|210997|British actor and...|British actor and...|https://en.wikine...|    1|\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SQLContext\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Clustering\")\\\n",
    "    .getOrCreate()\n",
    "# dataframe1 = spark.read.json('hdfs://localhost:1234/user/tl2861/hw3/train.json')\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "# dataframe1 = sqlContext.read.json('hdfs://localhost:1234/user/tl2861/hw3/train.json')\n",
    "dataframe1 = sqlContext.read.json('../../data/AA/wiki_*').withColumn('label',lit(1))\n",
    "dataframe2 = sqlContext.read.json('../../data/AAquote/wiki_*').withColumn('label', lit(2))\n",
    "dataframe3 = sqlContext.read.json('../../data/AAvoyage/wiki_*').withColumn('label',lit(3))\n",
    "dataframe = dataframe1.unionAll(dataframe2).unionAll(dataframe3)\n",
    "# dataframe1.printSchema()\n",
    "# dataframe2.printSchema()\n",
    "dataframe.printSchema()\n",
    "dataframe1.show()\n",
    "#regexTokenizer = RegexTokenizer()\n",
    "sampledData = dataframe.sampleBy(\"label\", fractions={1: 0.2, 2: 0.2, 3:0.2}, seed=0)\n",
    "sampledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 4381|\n",
      "|    2| 6586|\n",
      "|    3| 5687|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sampledData.createOrReplaceTempView(\"wikinews\")\n",
    "# # sampledData = spark.sql(\"select count(*) from wikinews\")\n",
    "# sampledData.show()\n",
    "sampledData.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"[^A-Za-z]+\", toLowercase=True)\n",
    "tokenizedData = regexTokenizer.transform(sampledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filteredData = stopWordsRemover.transform(tokenizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(filteredData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf= IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[13],[0.29251...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,2,3,4,5,6,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = idfModel.transform(featurizedData)\n",
    "datatext = data1.select('features')\n",
    "datatext.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 5588951.555903378\n",
      "Cluster Centers: \n",
      "[1.90218591 1.76718211 1.9384885  1.75734003 1.85523991 1.60344867\n",
      " 1.75974671 1.78994292 2.09537133 1.71989525 1.84462279 1.71652649\n",
      " 1.65677558 2.04217616 1.78857564 1.92950761 1.84122447 1.84168953\n",
      " 1.65142181 1.76094185]\n",
      "[216.93424333 226.97499146 236.95311999 224.68000297 247.23592687\n",
      " 186.44055826 226.9477241  247.24803096 260.64449734 215.07688574\n",
      " 230.91845654 224.74092657 214.78255033 276.18491303 226.63861283\n",
      " 209.70512765 240.29353518 262.07779673 249.83620712 230.74621583]\n",
      "[39.60137921 39.19407256 43.96566935 37.51279381 40.95349113 34.07051731\n",
      " 37.33200405 40.24226937 47.05695707 35.00688492 40.35540531 37.3945405\n",
      " 34.25163389 44.75369227 37.92117138 39.84130398 38.84469415 42.29642833\n",
      " 36.00683019 41.33687224]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "kmModel = kmeans.fit(data1)\n",
    "wssse = kmModel.computeCost(data1)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "     print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(kmModel.groupBy(kmModel.prediction).count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "|    1|         0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = kmModel.transform(data1).select(\"label\",\"prediction\")\n",
    "sorted(df.groupBy(df.age).count().collect())\n",
    "# transformed.corr(\"label\",\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "-473262.5193272425\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# loads data\n",
    "gmm = GaussianMixture().setK(3).setSeed(538009335)\n",
    "gmmModel = gmm.fit(data1)\n",
    "\n",
    "# print(\"Gaussians shown as a DataFrame: \")\n",
    "# model.gaussiansDF.show(truncate=False)\n",
    "\n",
    "summary = gmmModel.summary\n",
    "print (summary.k)\n",
    "print (summary.logLikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -2390009.871012725\n",
      "The upper bound on perplexity: 3.0418218773216754\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+----------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                     |\n",
      "+-----+-----------+----------------------------------------------------------------+\n",
      "|0    |[8, 13, 9] |[0.07043593730288787, 0.06712518106433367, 0.06675711484113477] |\n",
      "|1    |[3, 6, 4]  |[0.08853821689532505, 0.08585223821989096, 0.07149464974725137] |\n",
      "|2    |[2, 8, 10] |[0.062113445695059805, 0.0587201121505063, 0.055194423813886076]|\n",
      "+-----+-----------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=3, maxIter=10)\n",
    "ldaModel = lda.fit(data1)\n",
    "\n",
    "ll = ldaModel.logLikelihood(data1)\n",
    "lp = ldaModel.logPerplexity(data1)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = ldaModel.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# transformed = ldaModel.transform(data1)\n",
    "# transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
