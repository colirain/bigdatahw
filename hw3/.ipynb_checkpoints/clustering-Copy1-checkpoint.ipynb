{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clustering\n",
    "#### 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "|    id|                text|               title|                 url|label|\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "|205661|Australian rules ...|Australian rules ...|https://en.wikine...|    0|\n",
      "|206148|Pope Benedict XVI...|Pope Benedict XVI...|https://en.wikine...|    0|\n",
      "|206369|           Victoria\n",
      "|            Victoria|https://en.wikine...|    0|\n",
      "|206436|Gunman shoots doc...|Gunman shoots doc...|https://en.wikine...|    0|\n",
      "|207119|Pope Benedict XVI...|Pope Benedict XVI...|https://en.wikine...|    0|\n",
      "|207153|Deepwater Horizon...|Deepwater Horizon...|https://en.wikine...|    0|\n",
      "|207214|Thirty die in car...|Thirty die in car...|https://en.wikine...|    0|\n",
      "|207437|Emma's Imaginatio...|Emma's Imaginatio...|https://en.wikine...|    0|\n",
      "|207482|Common cold virus...|Common cold virus...|https://en.wikine...|    0|\n",
      "|207735|New Zealand man g...|New Zealand man g...|https://en.wikine...|    0|\n",
      "|208088|'Poetry lost': ru...|'Poetry lost': ru...|https://en.wikine...|    0|\n",
      "|208519|Man released foll...|Man released foll...|https://en.wikine...|    0|\n",
      "|208803|Fernando Alonso w...|Fernando Alonso w...|https://en.wikine...|    0|\n",
      "|208933|Owner of Segway J...|Owner of Segway J...|https://en.wikine...|    0|\n",
      "|209059|Southwest Airline...|Southwest Airline...|https://en.wikine...|    0|\n",
      "|209467|Wikinews Shorts: ...|Wikinews Shorts: ...|https://en.wikine...|    0|\n",
      "|209844|US undergraduate ...|US undergraduate ...|https://en.wikine...|    0|\n",
      "|210549|Plane crash on Ca...|Plane crash on Ca...|https://en.wikine...|    0|\n",
      "|210940|Bomb attack in Lo...|Bomb attack in Lo...|https://en.wikine...|    0|\n",
      "|210997|British actor and...|British actor and...|https://en.wikine...|    0|\n",
      "+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark import SQLContext\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Clustering\")\\\n",
    "    .getOrCreate()\n",
    "# dataframe1 = spark.read.json('hdfs://localhost:1234/user/tl2861/hw3/train.json')\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "# dataframe1 = sqlContext.read.json('hdfs://localhost:1234/user/tl2861/hw3/train.json')\n",
    "dataframe1 = sqlContext.read.json('../../../data/AA/wiki_*').withColumn('label',lit(0))\n",
    "dataframe2 = sqlContext.read.json('../../../data/AAquote/wiki_*').withColumn('label', lit(1))\n",
    "# dataframe3 = sqlContext.read.json('../../data/AAvoyage/wiki_*').withColumn('label',lit(3))\n",
    "dataframe = dataframe1.unionAll(dataframe2)\n",
    "# dataframe1.printSchema()\n",
    "# dataframe2.printSchema()\n",
    "dataframe.printSchema()\n",
    "# dataframe1.show()\n",
    "#regexTokenizer = RegexTokenizer()\n",
    "sampledData = dataframe.sampleBy(\"label\", fractions={0: 0.2, 1: 0.2}, seed=0)\n",
    "sampledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 4381|\n",
      "|    1| 6586|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sampledData.createOrReplaceTempView(\"wikinews\")\n",
    "# # sampledData = spark.sql(\"select count(*) from wikinews\")\n",
    "# sampledData.show()\n",
    "sampledData.groupBy(\"label\").count().orderBy(\"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"[^A-Za-z]+\", toLowercase=True)\n",
    "tokenizedData = regexTokenizer.transform(sampledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filteredData = stopWordsRemover.transform(tokenizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(filteredData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf= IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[13],[0.33957...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,2,3,4,5,6,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "|(20,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = idfModel.transform(featurizedData)\n",
    "datatext = data1.select('features')\n",
    "datatext.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 1335214.9809889607\n",
      "Cluster Centers: \n",
      "[24.70595957 21.86310698 28.09276079 24.66477462 31.0944174  18.56860122\n",
      " 22.48443747 19.4617212  23.95028956 18.58052216 23.5488747  21.43006121\n",
      " 17.48836579 26.64247329 20.18451575 24.49362609 19.96815644 22.04731022\n",
      " 19.89367364 21.86714012]\n",
      "[1.45820232 1.27541215 1.43199953 1.33120029 1.53704863 1.16367569\n",
      " 1.33369765 1.24050032 1.49248339 1.28095416 1.32525024 1.31310688\n",
      " 1.16756241 1.47920655 1.32047555 1.39996726 1.30438661 1.2579419\n",
      " 1.25805494 1.19740135]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(4381)\n",
    "kmModel = kmeans.fit(data1)\n",
    "wssse = kmModel.computeCost(data1)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "     print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0, prediction=0, count=54),\n",
       " Row(label=0, prediction=1, count=4327),\n",
       " Row(label=1, prediction=0, count=20),\n",
       " Row(label=1, prediction=1, count=6566)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = kmModel.transform(data1).select(\"label\",\"prediction\")\n",
    "sorted(transformed.groupBy(\"label\",\"prediction\").count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "-292533.9433002736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=0, prediction=0, count=1196),\n",
       " Row(label=0, prediction=1, count=3185),\n",
       " Row(label=1, prediction=0, count=117),\n",
       " Row(label=1, prediction=1, count=6469)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# loads data\n",
    "gmm = GaussianMixture().setK(2).setSeed(4381)\n",
    "gmmModel = gmm.fit(data1)\n",
    "# print(\"Gaussians shown as a DataFrame: \")\n",
    "# gmmModel.gaussiansDF.show(truncate=False)\n",
    "\n",
    "summary = gmmModel.summary\n",
    "print (summary.k)\n",
    "print (summary.logLikelihood)\n",
    "\n",
    "# data1.show()\n",
    "transformedgmm = gmmModel.transform(data1).select(\"label\",\"prediction\")\n",
    "# transformedgmm.show()\n",
    "sorted(transformedgmm.groupBy(\"label\",\"prediction\").count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bisecting k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 1355152.0316923182\n",
      "Cluster Centers: \n",
      "[1.42554001 1.2493039  1.39664989 1.30114302 1.50355881 1.14028601\n",
      " 1.30734205 1.21583944 1.45995393 1.25024047 1.29711273 1.27991896\n",
      " 1.1438812  1.4499552  1.29566149 1.3694119  1.27542987 1.23091231\n",
      " 1.22985767 1.17150975]\n",
      "[20.81343369 18.15699766 23.43497771 20.50728453 25.40647808 15.55658654\n",
      " 18.63406449 16.32445984 20.27985639 16.27313167 19.54410339 18.51448757\n",
      " 14.82471631 21.86818186 16.8706718  20.45309232 17.09459481 18.36609166\n",
      " 16.95863594 18.11677664]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=0, prediction=0, count=4302),\n",
       " Row(label=0, prediction=1, count=79),\n",
       " Row(label=1, prediction=0, count=6562),\n",
       " Row(label=1, prediction=1, count=24)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(2).setSeed(4381)\n",
    "bkmModel = bkm.fit(data1)\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = bkmModel.computeCost(data1)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "\n",
    "# Shows the result.\n",
    "print(\"Cluster Centers: \")\n",
    "centers = bkmModel.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    \n",
    "bkmResult = bkmModel.transform(data1).select(\"label\",\"prediction\")\n",
    "sorted(bkmResult.groupBy(\"label\",\"prediction\").count().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -1001828.5023802075\n",
      "The upper bound on perplexity: 3.067371744260949\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+------------+---------------------------------------------------------------+\n",
      "|topic|termIndices |termWeights                                                    |\n",
      "+-----+------------+---------------------------------------------------------------+\n",
      "|0    |[11, 13, 19]|[0.0673852360083209, 0.0619897702944972, 0.06059698785599344]  |\n",
      "|1    |[3, 2, 6]   |[0.07510775507942712, 0.06711253175693904, 0.06552518870667959]|\n",
      "|2    |[4, 8, 7]   |[0.12934398147584344, 0.07114973153278052, 0.0659543500889233] |\n",
      "+-----+------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=3, maxIter=10)\n",
    "ldaModel = lda.fit(data1)\n",
    "\n",
    "ll = ldaModel.logLikelihood(data1)\n",
    "lp = ldaModel.logPerplexity(data1)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = ldaModel.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "# transformed = ldaModel.transform(data1)\n",
    "# transformed.show(truncate=False)\n",
    "\n",
    "# ldaResult = ldaModel.transform(data1).select(\"label\",\"prediction\")\n",
    "# sorted(ldaResult.groupBy(\"prediction\").count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
